{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytube import YouTube\n",
    "# from moviepy.editor import *\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import yt_dlp\n",
    "# from yt_dlp import YoutubeDL\n",
    "# import moviepy.editor as mp\n",
    "\n",
    "# # Set yt-dlp as the backend for pafy\n",
    "# os.environ[\"PYPYDL_PYPI_PACKAGE\"] = \"yt-dlp\"\n",
    "\n",
    "# # Create a YoutubeDL instance and set it to the pafy backend\n",
    "# def new_pafy_backend():\n",
    "#     yt_dlp_options = {'format': 'bestaudio/best'}\n",
    "#     return YoutubeDL(yt_dlp_options)\n",
    "\n",
    "# pafy.backend_shared.backend = new_pafy_backend()\n",
    "\n",
    "# import pafy\n",
    "\n",
    "# def download_youtube_audio(youtube_url, output_path):\n",
    "#     try:\n",
    "#         # Create a pafy object\n",
    "#         video = pafy.new(youtube_url)\n",
    "\n",
    "#         # Select the best audio stream\n",
    "#         best_audio = video.getbestaudio()\n",
    "\n",
    "#         # Temporary filename\n",
    "#         temp_filename = 'temp_audio.' + best_audio.extension\n",
    "\n",
    "#         # Download the audio stream to a temporary file\n",
    "#         best_audio.download(filepath=temp_filename)\n",
    "\n",
    "#         # Convert the downloaded file to MP3\n",
    "#         clip = mp.AudioFileClip(temp_filename)\n",
    "#         clip.write_audiofile(output_path)\n",
    "\n",
    "#         # Close the clip to free resources\n",
    "#         clip.close()\n",
    "\n",
    "#         # Remove the temporary file\n",
    "#         os.remove(temp_filename)\n",
    "\n",
    "#         print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Example YouTube URL\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\"\n",
    "\n",
    "# # Output path for the converted MP3 file\n",
    "# output_path = 'output/testing.mp3'\n",
    "\n",
    "# download_youtube_audio(youtube_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_youtube_audio(youtube_url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytube\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yt = pytube.YouTube(\"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = yt.streams.filter(only_audio=True).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = yt.streams.filter(only_audio=True).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytube import YouTube\n",
    "# import ffmpeg\n",
    "\n",
    "# text = 'https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan'\n",
    "\n",
    "# yt = YouTube(text)\n",
    "\n",
    "# # https://github.com/pytube/pytube/issues/301\n",
    "# stream_url = yt.streams.all()[0].url  # Get the URL of the video stream\n",
    "\n",
    "# # Probe the audio streams (use it in case you need information like sample rate):\n",
    "# #probe = ffmpeg.probe(stream_url)\n",
    "# #audio_streams = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)\n",
    "# #sample_rate = audio_streams['sample_rate']\n",
    "\n",
    "# # Read audio into memory buffer.\n",
    "# # Get the audio using stdout pipe of ffmpeg sub-process.\n",
    "# # The audio is transcoded to PCM codec in WAC container.\n",
    "# audio, err = (\n",
    "#     ffmpeg\n",
    "#     .input(stream_url)\n",
    "#     .output(\"pipe:\", format='wav', acodec='pcm_s16le')  # Select WAV output format, and pcm_s16le auidio codec. My add ar=sample_rate\n",
    "#     .run(capture_stdout=True)\n",
    "# )\n",
    "\n",
    "# # Write the audio buffer to file for testing\n",
    "# with open('audio.wav', 'wb') as f:\n",
    "#     f.write(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yt_dlp\n",
    "# import os\n",
    "\n",
    "# def download_youtube_audio(youtube_url, output_path):\n",
    "#     try:\n",
    "#         # yt-dlp options to extract audio and convert to mp3\n",
    "#         ydl_opts = {\n",
    "#             'format': 'bestaudio/best',\n",
    "#             'postprocessors': [{\n",
    "#                 'key': 'FFmpegExtractAudio',\n",
    "#                 'preferredcodec': 'mp3',\n",
    "#                 'preferredquality': '192',\n",
    "#             }],\n",
    "#             'outtmpl': 'temp_audio.%(ext)s'\n",
    "#         }\n",
    "\n",
    "#         # Download the audio using yt-dlp\n",
    "#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#             ydl.download([youtube_url])\n",
    "\n",
    "#         # Rename the downloaded file to the output path\n",
    "#         temp_filename = 'temp_audio.mp3'\n",
    "#         os.rename(temp_filename, output_path)\n",
    "#         print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Example YouTube URL\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\"\n",
    "\n",
    "# # Output path for the converted MP3 file\n",
    "# output_path = 'audio.mp3'\n",
    "\n",
    "# download_youtube_audio(youtube_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yt_dlp\n",
    "# import os\n",
    "\n",
    "# def download_youtube_audio(youtube_url, output_path):\n",
    "#     try:\n",
    "#         # Path to ffmpeg binary\n",
    "#         ffmpeg_path = r'C:\\Users\\AA\\Downloads\\ffmpeg-7.0.1-essentials_build\\ffmpeg-7.0.1-essentials_build\\bin\\ffmpeg.exe'\n",
    "\n",
    "#         # yt-dlp options to extract audio and convert to mp3\n",
    "#         ydl_opts = {\n",
    "#             'format': 'bestaudio/best',\n",
    "#             'postprocessors': [{\n",
    "#                 'key': 'FFmpegExtractAudio',\n",
    "#                 'preferredcodec': 'mp3',\n",
    "#                 'preferredquality': '192',\n",
    "#             }],\n",
    "#             'outtmpl': 'temp_audio.%(ext)s',\n",
    "#             'ffmpeg_location': ffmpeg_path\n",
    "#         }\n",
    "\n",
    "#         # Download the audio using yt-dlp\n",
    "#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#             ydl.download([youtube_url])\n",
    "\n",
    "#         # Rename the downloaded file to the output path\n",
    "#         temp_filename = 'temp_audio.mp3'\n",
    "#         os.rename(temp_filename, output_path)\n",
    "#         print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Example YouTube URL\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\"\n",
    "\n",
    "# # Output path for the converted MP3 file\n",
    "# output_path = 'audio.mp3'\n",
    "\n",
    "# download_youtube_audio(youtube_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yt_dlp\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg_path = r'C:\\Users\\AA\\Downloads\\ffmpeg-7.0.1-essentials_build\\ffmpeg-7.0.1-essentials_build\\bin\\ffmpeg.exe'\n",
    "\n",
    "#         # yt-dlp options to extract audio and convert to mp3\n",
    "# ydl_opts = {\n",
    "#             'format': 'bestaudio/best',\n",
    "#             'postprocessors': [{\n",
    "#                 'key': 'FFmpegExtractAudio',\n",
    "#                 'preferredcodec': 'mp3',\n",
    "#                 'preferredquality': '192',\n",
    "#             }],\n",
    "#             'outtmpl': 'temp_audio.%(ext)s',\n",
    "#             'ffmpeg_location': ffmpeg_path\n",
    "#         }\n",
    "\n",
    "#         # Download the audio using yt-dlp\n",
    "# with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#     ydl.download([\"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yt_dlp\n",
    "# import imageio_ffmpeg as ffmpeg\n",
    "# import os\n",
    "\n",
    "# def download_youtube_audio(youtube_url, output_path):\n",
    "#     try:\n",
    "#         # Get the ffmpeg binary path from imageio_ffmpeg\n",
    "#         ffmpeg_path = ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "#         # yt-dlp options to extract audio and convert to mp3\n",
    "#         ydl_opts = {\n",
    "#             'format': 'bestaudio/best',\n",
    "#             'postprocessors': [{\n",
    "#                 'key': 'FFmpegExtractAudio',\n",
    "#                 'preferredcodec': 'mp3',\n",
    "#                 'preferredquality': '192',\n",
    "#             }],\n",
    "#             'outtmpl': 'temp_audio.%(ext)s',\n",
    "#             'ffmpeg_location': ffmpeg_path,\n",
    "#             'verbose': True  # Enable verbose logging for debugging\n",
    "#         }\n",
    "\n",
    "#         # Download the audio using yt-dlp\n",
    "#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#             ydl.download([youtube_url])\n",
    "\n",
    "#         # Rename the downloaded file to the output path\n",
    "#         temp_filename = 'temp_audio.mp3'\n",
    "#         os.rename(temp_filename, output_path)\n",
    "#         print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Example YouTube URL\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=opNYFJ5SO10&ab_channel=CodingWithEvan\"\n",
    "\n",
    "# # Output path for the converted MP3 file\n",
    "# output_path = 'audio.mp3'\n",
    "\n",
    "# download_youtube_audio(youtube_url, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is Working below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking  e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win64-v4.2.2.exe\n",
      "Download and conversion complete. File saved as audio.mp3\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import imageio_ffmpeg as ffmpeg\n",
    "\n",
    "def download_youtube_audio(youtube_url, output_path):\n",
    "    try:\n",
    "        # Path to ffmpeg executable (if necessary, otherwise ffmpeg should be in PATH)\n",
    "        ffmpeg_path = ffmpeg.get_ffmpeg_exe()\n",
    "        print(\"checking \", ffmpeg_path)\n",
    "        # Construct the yt-dlp command\n",
    "        cmd = [\n",
    "            'yt-dlp',\n",
    "            '-x',  # Extract audio only\n",
    "            '--audio-format', 'mp3',\n",
    "            '--ffmpeg-location', ffmpeg_path,\n",
    "            '-o', output_path,  # Output file template\n",
    "            youtube_url  # YouTube video URL\n",
    "        ]\n",
    "\n",
    "        # Run the command using subprocess\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            print(\"checking resturn.returncode \", result.returncode)\n",
    "            print(f\"Error occurred: {result.stderr}\")\n",
    "        else:\n",
    "            print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error checking \")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Example YouTube URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=x7X9w_GIm1s&ab_channel=Fireship\"\n",
    "\n",
    "# Output path for the converted MP3 file\n",
    "output_path = 'extracted_audio/audio.mp3'\n",
    "\n",
    "download_youtube_audio(youtube_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "# import imageio_ffmpeg as ffmpeg\n",
    "# import yt_dlp\n",
    "\n",
    "# def download_youtube_audio_playlist(playlist_url, output_dir):\n",
    "#     try:\n",
    "#         # Path to ffmpeg executable (if necessary, otherwise ffmpeg should be in PATH)\n",
    "#         ffmpeg_path = ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "#         # Create output directory if it doesn't exist\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "\n",
    "#         # Options to retrieve video info from playlist\n",
    "#         ydl_opts = {\n",
    "#             'quiet': True,\n",
    "#             'extract_flat': True,\n",
    "#             'force_generic_extractor': True,\n",
    "#         }\n",
    "\n",
    "#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#             playlist_info = ydl.extract_info(playlist_url, download=False)\n",
    "\n",
    "#         # Check if the URL is a playlist and has entries\n",
    "#         if 'entries' not in playlist_info:\n",
    "#             print(\"The provided URL is not a playlist or has no entries.\")\n",
    "#             return\n",
    "\n",
    "#         # Iterate through each video in the playlist\n",
    "#         for index, entry in enumerate(playlist_info['entries'], start=1):\n",
    "#             video_url = entry['url']\n",
    "#             output_path = os.path.join(output_dir, f'audio{index}.mp3')\n",
    "\n",
    "#             # Construct the yt-dlp command\n",
    "#             cmd = [\n",
    "#                 'yt-dlp',\n",
    "#                 '-x',  # Extract audio only\n",
    "#                 '--audio-format', 'mp3',\n",
    "#                 '--ffmpeg-location', ffmpeg_path,\n",
    "#                 '-o', output_path,  # Output file template\n",
    "#                 video_url  # YouTube video URL\n",
    "#             ]\n",
    "\n",
    "#             # Run the command using subprocess\n",
    "#             result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "#             # Check for errors\n",
    "#             if result.returncode != 0:\n",
    "#                 print(f\"Error occurred for {video_url}: {result.stderr}\")\n",
    "#             else:\n",
    "#                 print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Example YouTube playlist URL\n",
    "# playlist_url = \"https://youtube.com/playlist?list=PLI-hcKnojs-hM3Dtec1kyP2gym7sghNFR&si=bJn2d1cvuOlb10qC\"\n",
    "\n",
    "# # Output directory for the converted MP3 files\n",
    "# output_dir = 'extracted_audio'\n",
    "\n",
    "# download_youtube_audio_playlist(playlist_url, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For playlist download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'list'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AA\\AppData\\Local\\Temp\\ipykernel_15156\\4028681533.py\", line 17, in download_youtube_audio_playlist\n",
      "    video_urls = [video.watch_url for video in playlist.videos]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\helpers.py\", line 71, in __iter__\n",
      "    curr_item = self[iter_index]\n",
      "                ~~~~^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\helpers.py\", line 57, in __getitem__\n",
      "    next_item = next(self.gen)\n",
      "                ^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 296, in videos_generator\n",
      "    for url in self.video_urls:\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\helpers.py\", line 71, in __iter__\n",
      "    curr_item = self[iter_index]\n",
      "                ~~~~^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\helpers.py\", line 57, in __getitem__\n",
      "    next_item = next(self.gen)\n",
      "                ^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 281, in url_generator\n",
      "    for page in self._paginate():\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 118, in _paginate\n",
      "    json.dumps(extract.initial_data(self.html))\n",
      "                                    ^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 58, in html\n",
      "    self._html = request.get(self.playlist_url)\n",
      "                             ^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 48, in playlist_url\n",
      "    return f\"https://www.youtube.com/playlist?list={self.playlist_id}\"\n",
      "                                                    ^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\contrib\\playlist.py\", line 39, in playlist_id\n",
      "    self._playlist_id = extract.playlist_id(self._input_url)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Projects_working\\Audio_transcript\\.venv\\Lib\\site-packages\\pytube\\extract.py\", line 151, in playlist_id\n",
      "    return parse_qs(parsed.query)['list'][0]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'list'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import imageio_ffmpeg as ffmpeg\n",
    "from pytube import Playlist\n",
    "\n",
    "def download_youtube_audio_playlist(playlist_url, output_dir):\n",
    "    try:\n",
    "        # Path to ffmpeg executable\n",
    "        ffmpeg_path = ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Get video URLs from the playlist using pytube\n",
    "        playlist = Playlist(playlist_url)\n",
    "        video_urls = [video.watch_url for video in playlist.videos]\n",
    "\n",
    "        # Iterate through each video in the playlist\n",
    "        for index, video_url in enumerate(video_urls, start=1):\n",
    "            output_path = os.path.join(output_dir, f'audio{index}.mp3')\n",
    "\n",
    "            # Construct the yt-dlp command\n",
    "            cmd = [\n",
    "                'yt-dlp',\n",
    "                '-x',  # Extract audio only\n",
    "                '--audio-format', 'mp3',\n",
    "                '--ffmpeg-location', ffmpeg_path,\n",
    "                '-o', output_path,  # Output file template\n",
    "                video_url  # YouTube video URL\n",
    "            ]\n",
    "\n",
    "            # Run the command using subprocess\n",
    "            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "            # Check for errors\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error occurred for {video_url}: {result.stderr}\")\n",
    "            else:\n",
    "                print(f\"Download and conversion complete. File saved as {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Example YouTube playlist URL\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PLI-hcKnojs-hM3Dtec1kyP2gym7sghNFR\"\n",
    "\n",
    "# Output directory for the converted MP3 files\n",
    "output_dir = 'extracted_audio'\n",
    "\n",
    "download_youtube_audio_playlist(playlist_url, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Transcript Vosk is working very poor trying to shift to deepspeesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pydub import AudioSegment\n",
    "# import wave\n",
    "# import json\n",
    "# from vosk import Model, KaldiRecognizer\n",
    "\n",
    "# def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
    "#     audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "#     print(\"checking audio load \", audio)\n",
    "#     audio.export(wav_file_path, format=\"wav\")\n",
    "#     print(f\"Converted {mp3_file_path} to {wav_file_path}\")\n",
    "\n",
    "# def transcribe_audio(wav_file_path, model):\n",
    "#     wf = wave.open(wav_file_path, \"rb\")\n",
    "#     print(\"checking wf \", wf)\n",
    "#     rec = KaldiRecognizer(model, wf.getframerate())\n",
    "#     rec.SetWords(True)\n",
    "#     results = []\n",
    "#     while True:\n",
    "#         data = wf.readframes(4000)\n",
    "#         if len(data) == 0:\n",
    "#             break\n",
    "#         if rec.AcceptWaveform(data):\n",
    "#             results.append(json.loads(rec.Result()))\n",
    "#         else:\n",
    "#             results.append(json.loads(rec.PartialResult()))\n",
    "#     results.append(json.loads(rec.FinalResult()))\n",
    "#     text = ' '.join([res['text'] for res in results if 'text' in res])\n",
    "#     print(f\"Transcription for {wav_file_path}:\")\n",
    "#     print(text)\n",
    "#     return text\n",
    "\n",
    "# def process_audio_files(input_dir, output_dir, model_path):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     # Verify model path\n",
    "#     if not os.path.exists(model_path):\n",
    "#         raise Exception(f\"Model path {model_path} does not exist. Please check the path and try again.\")\n",
    "    \n",
    "#     print(f\"Loading Vosk model from {model_path}...\")\n",
    "\n",
    "#     try:\n",
    "#         print(\"model creation checking \")\n",
    "#         # Load the Vosk model\n",
    "#         model = Model(model_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to create a model from the path {model_path}. Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     for index, audio_file in enumerate(os.listdir(input_dir), start=1):\n",
    "#         if audio_file.endswith('.mp3'):\n",
    "#             mp3_file_path = os.path.join(input_dir, audio_file)\n",
    "#             wav_file_path = os.path.join(output_dir, f'audio{index}.wav')\n",
    "#             convert_mp3_to_wav(mp3_file_path, wav_file_path)\n",
    "#             transcription = transcribe_audio(wav_file_path, model)\n",
    "#             text_file_path = os.path.join(output_dir, f'textfile{index}.txt')\n",
    "#             with open(text_file_path, 'w') as f:\n",
    "#                 f.write(transcription)\n",
    "#             print(f\"Transcription saved to {text_file_path}\")\n",
    "\n",
    "# # Directories\n",
    "# input_dir = 'extracted_audio'\n",
    "# output_dir = 'transcription_results'\n",
    "# model_path = 'audio_model/vosk-model-en-us-0.42-gigaspeech'  # Replace with your Vosk model path\n",
    "\n",
    "# process_audio_files(input_dir, output_dir, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DeepSpeech by Mozila taking too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepSpeech model from audio_model/deepspeech-0.9.3-models.pbmm...\n",
      "Loading DeepSpeech scorer from audio_model/deepspeech-0.9.3-models.scorer...\n",
      "checking audio load  <pydub.audio_segment.AudioSegment object at 0x000001C2AD27A130>\n",
      "Converted extracted_audio\\audio.mp3 to transcription_results\\audio1.wav\n",
      "Starting transcription for transcription_results\\audio1.wav...\n",
      "Enabling scorer...\n",
      "Running model.stt...\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pydub import AudioSegment\n",
    "# import wave\n",
    "# import numpy as np\n",
    "# import deepspeech\n",
    "\n",
    "# def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
    "#     audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "#     print(\"checking audio load \", audio)\n",
    "#     audio.export(wav_file_path, format=\"wav\")\n",
    "#     print(f\"Converted {mp3_file_path} to {wav_file_path}\")\n",
    "\n",
    "# def transcribe_audio(wav_file_path, model, scorer):\n",
    "#     print(f\"Starting transcription for {wav_file_path}...\")\n",
    "#     wf = wave.open(wav_file_path, \"rb\")\n",
    "#     fs = wf.getframerate()\n",
    "#     audio = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)\n",
    "    \n",
    "#     print(\"Enabling scorer...\")\n",
    "#     model.enableExternalScorer(scorer)\n",
    "#     print(\"Running model.stt...\")\n",
    "#     text = model.stt(audio)\n",
    "    \n",
    "#     print(f\"Transcription for {wav_file_path}:\")\n",
    "#     print(text)\n",
    "#     return text\n",
    "\n",
    "# def process_audio_files(input_dir, output_dir, model_path, scorer_path):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     print(f\"Loading DeepSpeech model from {model_path}...\")\n",
    "#     model = deepspeech.Model(model_path)\n",
    "#     print(f\"Loading DeepSpeech scorer from {scorer_path}...\")\n",
    "#     model.enableExternalScorer(scorer_path)\n",
    "    \n",
    "#     for index, audio_file in enumerate(os.listdir(input_dir), start=1):\n",
    "#         if audio_file.endswith('.mp3'):\n",
    "#             mp3_file_path = os.path.join(input_dir, audio_file)\n",
    "#             wav_file_path = os.path.join(output_dir, f'audio{index}.wav')\n",
    "#             convert_mp3_to_wav(mp3_file_path, wav_file_path)\n",
    "#             transcription = transcribe_audio(wav_file_path, model, scorer_path)\n",
    "#             text_file_path = os.path.join(output_dir, f'textfile{index}.txt')\n",
    "#             with open(text_file_path, 'w') as f:\n",
    "#                 f.write(transcription)\n",
    "#             print(f\"Transcription saved to {text_file_path}\")\n",
    "\n",
    "# # Directories\n",
    "# input_dir = 'extracted_audio'\n",
    "# output_dir = 'transcription_results'\n",
    "# model_path = 'audio_model/deepspeech-0.9.3-models.pbmm'  # Replace with your DeepSpeech model path\n",
    "# scorer_path = 'audio_model/deepspeech-0.9.3-models.scorer'  # Replace with your DeepSpeech scorer path\n",
    "\n",
    "# process_audio_files(input_dir, output_dir, model_path, scorer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using amazon Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket audiootextai already exists.\n",
      "File extracted_audio/audio1.mp3 uploaded to S3 bucket audiootextai with key audio1.mp3\n",
      "Error starting transcription job: An error occurred (ConflictException) when calling the StartTranscriptionJob operation: The requested job name already exists. Use a different job name.\n",
      "Transcription completed. Transcript URI: https://s3.us-east-1.amazonaws.com/audiootextai/transcription-job-1.json\n",
      "Transcription result available at: https://s3.us-east-1.amazonaws.com/audiootextai/transcription-job-1.json\n",
      "Transcript downloaded to transcription_results/transcript1.json\n",
      "Transcript saved to transcription_results/transcript1.txt\n",
      "File extracted_audio/audio2.mp3 uploaded to S3 bucket audiootextai with key audio2.mp3\n",
      "Error starting transcription job: An error occurred (ConflictException) when calling the StartTranscriptionJob operation: The requested job name already exists. Use a different job name.\n",
      "Transcription completed. Transcript URI: https://s3.us-east-1.amazonaws.com/audiootextai/transcription-job-2.json\n",
      "Transcription result available at: https://s3.us-east-1.amazonaws.com/audiootextai/transcription-job-2.json\n",
      "Transcript downloaded to transcription_results/transcript2.json\n",
      "Transcript saved to transcription_results/transcript2.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Initialize Boto3 clients\n",
    "# Use a specific profile\n",
    "session = boto3.Session(profile_name='default')\n",
    "s3_client = session.client('s3')\n",
    "transcribe_client = session.client('transcribe')\n",
    "\n",
    "# Constants\n",
    "bucket_name = 'audiootextai'  # Replace with your S3 bucket name\n",
    "region = 'us-east-1'  # Replace with your AWS region\n",
    "\n",
    "def create_s3_bucket(bucket_name, region):\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration=location\n",
    "            )\n",
    "        print(f\"Bucket {bucket_name} created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bucket {bucket_name}: {e}\")\n",
    "\n",
    "def upload_file_to_s3(file_path, bucket_name, s3_key):\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        print(f\"File {file_path} uploaded to S3 bucket {bucket_name} with key {s3_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "def start_transcription_job(job_name, s3_uri, language_code='en-US'):\n",
    "    try:\n",
    "        response = transcribe_client.start_transcription_job(\n",
    "            TranscriptionJobName=job_name,\n",
    "            Media={'MediaFileUri': s3_uri},\n",
    "            MediaFormat='mp3',\n",
    "            LanguageCode=language_code,\n",
    "            OutputBucketName=bucket_name\n",
    "        )\n",
    "        print(f\"Started transcription job with name: {job_name}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting transcription job: {e}\")\n",
    "\n",
    "def get_transcription_result(job_name):\n",
    "    while True:\n",
    "        response = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
    "        status = response['TranscriptionJob']['TranscriptionJobStatus']\n",
    "        if status in ['COMPLETED', 'FAILED']:\n",
    "            break\n",
    "        print(f\"Transcription job {job_name} status: {status}\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    if status == 'COMPLETED':\n",
    "        transcript_uri = response['TranscriptionJob']['Transcript']['TranscriptFileUri']\n",
    "        print(f\"Transcription completed. Transcript URI: {transcript_uri}\")\n",
    "        return transcript_uri\n",
    "    else:\n",
    "        print(f\"Transcription job {job_name} failed.\")\n",
    "        return None\n",
    "\n",
    "def download_transcript(transcript_uri, output_path):\n",
    "    try:\n",
    "        response = requests.get(transcript_uri)\n",
    "        response.raise_for_status()\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(response.json(), f)\n",
    "        print(f\"Transcript downloaded to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading transcript: {e}\")\n",
    "\n",
    "def extract_transcript_from_file(json_file_path, output_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            # Extracting the transcript text\n",
    "            transcript_text = data['results']['transcripts'][0]['transcript']\n",
    "            \n",
    "            # Save the clean transcript to a text file\n",
    "            with open(output_file_path, 'w') as text_file:\n",
    "                text_file.write(transcript_text)\n",
    "            print(f\"Transcript saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {json_file_path}: {e}\")\n",
    "\n",
    "def main():\n",
    "    file_paths = ['extracted_audio/audio1.mp3', 'extracted_audio/audio2.mp3']  # Replace with your list of MP3 file paths\n",
    "\n",
    "    # Check if the bucket exists and create it if not\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except:\n",
    "        print(f\"Bucket {bucket_name} does not exist. Creating bucket...\")\n",
    "        create_s3_bucket(bucket_name, region)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('transcription_results', exist_ok=True)\n",
    "\n",
    "    for i, file_path in enumerate(file_paths, start=1):\n",
    "        s3_key = f'audio{i}.mp3'\n",
    "        job_name = f'transcription-job-{i}'\n",
    "        transcript_output_path = f'transcription_results/transcript{i}.json'\n",
    "        clean_text_output_path = f'transcription_results/transcript{i}.txt'\n",
    "\n",
    "        # Upload the file to S3\n",
    "        upload_file_to_s3(file_path, bucket_name, s3_key)\n",
    "\n",
    "        # Start transcription job\n",
    "        s3_uri = f's3://{bucket_name}/{s3_key}'\n",
    "        start_transcription_job(job_name, s3_uri)\n",
    "\n",
    "        # Get transcription result\n",
    "        transcript_uri = get_transcription_result(job_name)\n",
    "        if transcript_uri:\n",
    "            print(f\"Transcription result available at: {transcript_uri}\")\n",
    "            # Download the transcription result\n",
    "            download_transcript(transcript_uri, transcript_output_path)\n",
    "            # Extract clean text and save to file\n",
    "            extract_transcript_from_file(transcript_output_path, clean_text_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model setup/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# # from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_text_splitters.character import CharacterTextSplitter\n",
    "\n",
    "# persist_directory = './chroma_db'\n",
    "\n",
    "# transcription_dir = \"./transcription_results\"\n",
    "# for file_name in os.listdir(transcription_dir):\n",
    "#         if file_name.endswith('.txt'):\n",
    "#             file_path = os.path.join(transcription_dir, file_name)\n",
    "#             loader = TextLoader(file_path)\n",
    "#             documents = loader.load()\n",
    "#             text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "#             docs = text_splitter.split_documents(documents)\n",
    "#             hf_embeddings.embed_documents(docs)\n",
    "\n",
    "# vectordb = Chroma.from_documents(documents=docs, embedding=hf_embedding, persist_directory=persist_directory)\n",
    "# # persiste the db to disk\n",
    "# vectordb.persist()\n",
    "\n",
    "# retriever = vectordb.as_retriever()\n",
    "# query = \"Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump.\"\n",
    "# docs = retriever.get_relevant_documents(query)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma db setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "\n",
    "# Directory for Chroma database\n",
    "persist_directory = './chroma_db'\n",
    "\n",
    "# Directory containing transcription results\n",
    "transcription_dir = \"./transcription_results\"\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "# Load and process documents\n",
    "for file_name in os.listdir(transcription_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(transcription_dir, file_name)\n",
    "        loader = TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "# Initialize Chroma vector database with the documents\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_docs,\n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"what is the most popular language in the world?\"\n",
    "# # docs = retriever.get_relevant_documents(query)\n",
    "# result = vectordb.similarity_search(query)\n",
    "\n",
    "# # Print the content of the first relevant document\n",
    "# if result:\n",
    "#     print(result[0].page_content)\n",
    "# else:\n",
    "#     print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'llm_model'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'llm_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from E:\\HuggingFaceCache\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-Instruct-GGUF\\snapshots\\f0377f2e99a3bf6d3cb94091e9f5488b3b0c5855\\Meta-Llama-3.1-8B-Instruct.Q5_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.21 GiB (5.57 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5332.43 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3.1', 'llama.block_count': '32', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '8', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'smaug-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =    3527.47 ms\n",
      "llama_print_timings:      sample time =      71.25 ms /   506 runs   (    0.14 ms per token,  7102.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3527.40 ms /     6 tokens (  587.90 ms per token,     1.70 tokens per second)\n",
      "llama_print_timings:        eval time =  280109.82 ms /   505 runs   (  554.67 ms per token,     1.80 tokens per second)\n",
      "llama_print_timings:       total time =  285202.88 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.\" - Douglas Adams\n",
      "\"Life is like a game of Jenga. You pull out a piece, and you never know what's going to happen.\" - Unknown\n",
      "\"Life is a series of choices, and each choice we make leads us down a different path.\" - Unknown\n",
      "\"Life is a journey, not a destination.\" - Ralph Waldo Emerson\n",
      "\"Life is a canvas, and we are the artists who paint it with our choices and actions.\" - Unknown\n",
      "\"Life is a puzzle, and we are the pieces that fit together to create a beautiful picture.\" - Unknown\n",
      "\"Life is a song, and we are the melody that makes it beautiful.\" - Unknown\n",
      "\"Life is a dance, and we are the steps that move to the rhythm of our hearts.\" - Unknown\n",
      "\"Life is a story, and we are the authors who write it with our experiences and lessons.\" - Unknown\n",
      "\"Life is a journey, and we are the travelers who explore its wonders and challenges.\" - Unknown\n",
      "\"Life is a tapestry, and we are the threads that weave it into a beautiful fabric.\" - Unknown\n",
      "\"Life is a river, and we are the boats that sail on its waters, carrying our hopes and dreams.\" - Unknown\n",
      "\"Life is a garden, and we are the gardeners who nurture it with our love and care.\" - Unknown\n",
      "\"Life is a flame, and we are the sparks that ignite it with our passions and creativity.\" - Unknown\n",
      "\"Life is a dream, and we are the dreamers who make it a reality with our hard work and determination.\" - Unknown\n",
      "\"Life is a journey, and we are the travelers who explore its wonders and challenges.\" - Unknown\n",
      "\"Life is a tapestry, and we are the threads that weave it into a beautiful fabric.\" - Unknown\n",
      "\"Life is a river, and we are the boats that sail on its waters, carrying our hopes and dreams.\" - Unknown\n",
      "\"Life is a garden, and we are the gardeners who nurture it with our love and care.\" - Unknown\n",
      "\"Life is a flame, and we are the sparks that ignite it with our passions and creativity.\" - Unknown\n",
      "\"Life is a dream, and we are the dreamers who make it a reality with our hard work and determination.\" - Unknown\n",
      "\"Life is a journey, and we are the travelers who explore its wonders and challenges.\" - Unknown\n",
      "\"Life is a tapestry, and we are the threads\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "## Download the GGUF model\n",
    "model_name = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "model_file = \"Meta-Llama-3.1-8B-Instruct.Q5_0.gguf\"\n",
    "# model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\"\n",
    "# model_file = \"mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
    "model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=512,  # Further reduced context length to manage memory\n",
    "    n_threads=8,  # Number of CPU threads to use\n",
    "    n_gpu_layers=0  # Number of model layers to offload to GPU (set to 0 if GPU is not suitable)\n",
    ")\n",
    "\n",
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":20000,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "prompt = \"The meaning of life is \"\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res[\"choices\"][0][\"text\"])\n",
    "# res is short for result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# model = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "# model_file = \"Meta-Llama-3.1-8B-Instruct.Q5_0.gguf\"\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\", model_file=model_file, hf=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llama_model_id = \"QuantFactory/Meta-Llama-3-70B-Instruct-GGUF\"\n",
    "# llama_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=llama_model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device=\"auto\",\n",
    "# )\n",
    "\n",
    "# def generate_response(context, question):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpfull assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": context + \"\\n\" + question},\n",
    "#     ]\n",
    "#     prompt = llama_pipeline.tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     )\n",
    "#     terminators = [\n",
    "#         llama_pipeline.tokenizer.eos_token_id,\n",
    "#         llama_pipeline.tokenizer.convert_tokens_to_ids(\"\")\n",
    "#     ]\n",
    "#     outputs = llama_pipeline(\n",
    "#         prompt,\n",
    "#         max_new_tokens=256,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.6,\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "#     return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "# def answer_question(question):\n",
    "#     docs = db.similarity_search(question)\n",
    "#     context = \" \".join([doc.page_content for doc in docs])\n",
    "#     response = generate_response(context, question)\n",
    "#     return response\n",
    "\n",
    "# # Example usage\n",
    "# question = \"What is the main topic of the first lecture?\"\n",
    "# answer = answer_question(question)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import os\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# os.environ['HF_HOME'] = './llm_model'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = './llm_model'\n",
    "\n",
    "# # Model and tokenizer setup\n",
    "# model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "# filename = \"Meta-Llama-3-8B-Instruct-v2.Q4_0.gguf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"llm_model\\Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import os\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# os.environ['HF_HOME'] = './llm_model'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = './llm_model'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# # Check if the model is loaded correctly\n",
    "# print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from ollama import OllamaModel, OllamaTokenizer\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# os.environ['OLLAMA_HOME'] = './llm_model'\n",
    "# os.environ['OLLAMA_CACHE'] = './llm_model'\n",
    "\n",
    "# # Path to the model files\n",
    "# model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "# filename = \"Meta-Llama-3-8B-Instruct-v2.Q4_0.gguf\"\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# tokenizer = OllamaTokenizer.from_pretrained(model_id, cache_dir=\"./llm_model\")\n",
    "# model = OllamaModel.from_pretrained(model_id, gguf_file=filename, cache_dir=\"./llm_model\")\n",
    "\n",
    "# # Define a prompt for inference\n",
    "# prompt = \"What is the capital of France?\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # Generate a response\n",
    "# output = model.generate(input_ids=input_ids, max_length=50)\n",
    "\n",
    "# # Decode and print the response\n",
    "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "# import os\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# os.environ['HF_HOME'] = './llm_model'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = './llm_model'\n",
    "\n",
    "# # Model and tokenizer setup\n",
    "# model_id = \"openbmb/MiniCPM-Llama3-V-2_5-int4\"  # Example model ID for a non-quantized model\n",
    "# # If you have a quantized model, replace with the appropriate ID and filename\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# # os.environ['HF_HOME'] = './llm_model'\n",
    "# # os.environ['TRANSFORMERS_CACHE'] = './llm_model'\n",
    "\n",
    "# # Model and tokenizer setup\n",
    "# model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "# filename = \"Meta-Llama-3-8B-Instruct-v2.Q4_0.gguf\"\n",
    "\n",
    "# # Load the model and tokenizer using ctransformers\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, model_file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# # os.environ['HF_HOME'] = './llm_model'\n",
    "# # os.environ['TRANSFORMERS_CACHE'] = './llm_model'\n",
    "\n",
    "# # Model and tokenizer setup\n",
    "# model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "# filename = \"Meta-Llama-3-8B-Instruct-v2.Q4_0.gguf\"\n",
    "\n",
    "# # Load the model and tokenizer using ctransformers\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, model_file=filename)\n",
    "\n",
    "# # Function to generate text\n",
    "# def generate_text(model, tokenizer, prompt):\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#     output = model.generate(input_ids)\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return response\n",
    "\n",
    "# # Test the model\n",
    "# prompt = \"hello\"\n",
    "# response = generate_text(model, tokenizer, prompt)\n",
    "# print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cpp import Llama\n",
    "# import os\n",
    "\n",
    "# # Set environment variables for cache directories\n",
    "# # os.environ['LLAMA_HOME'] = './llm_model'\n",
    "\n",
    "# # Define the path to the GGUF model\n",
    "# model_path = \"E:\\HuggingFaceCache\\hub\\models--QuantFactory--Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "\n",
    "# # Load the model using Llama\n",
    "# llama = Llama(model_path)\n",
    "\n",
    "# # Define the prompt\n",
    "# prompt = \"You are a helpful assistant. What is the capital of France?\"\n",
    "\n",
    "# # Generate text from the model\n",
    "# response = llama(prompt)\n",
    "\n",
    "# # Print the response\n",
    "# print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model_id = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "\n",
    "# outputs = pipeline(\n",
    "#     messages,\n",
    "#     max_new_tokens=256,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"llm_model/Meta-Llama-3-8B-Instruct.Q4_0.gguf\",\n",
    "#     model_type=\"llama-3\",\n",
    "# )\n",
    "\n",
    "# print(llm(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate text\n",
    "# prompt = \"You are a helpful assistant. Write a limerick about Python exceptions.\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate the response\n",
    "# output = model.generate(input_ids, max_length=100)\n",
    "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading chroma db which is already setup/created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    ")\n",
    "\n",
    "persist_directory = './chroma_db'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "# model_file = \"Meta-Llama-3.1-8B-Instruct.Q5_0.gguf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Create a pipeline for the HuggingFace model\n",
    "# generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# llm = HuggingFacePipeline(pipeline=generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading llama 3.1 quantized model which is already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from E:\\HuggingFaceCache\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-Instruct-GGUF\\snapshots\\f0377f2e99a3bf6d3cb94091e9f5488b3b0c5855\\Meta-Llama-3.1-8B-Instruct.Q5_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.21 GiB (5.57 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5332.43 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3.1', 'llama.block_count': '32', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '8', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'smaug-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# from llama_cpp import Llama\n",
    "\n",
    "# model_name = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "# model_file = \"Meta-Llama-3.1-8B-Instruct.Q5_0.gguf\"\n",
    "# model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "# llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_ctx=512,\n",
    "#     n_threads=8,\n",
    "#     n_gpu_layers=0,\n",
    "#     verbose=True,\n",
    "#     max_tokens=4000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from E:\\HuggingFaceCache\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-Instruct-GGUF\\snapshots\\f0377f2e99a3bf6d3cb94091e9f5488b3b0c5855\\Meta-Llama-3.1-8B-Instruct.Q5_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.21 GiB (5.57 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5332.43 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 2016\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   252.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  252.00 MiB, K (f16):  126.00 MiB, V (f16):  126.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.16 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3.1', 'llama.block_count': '32', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '8', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'smaug-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "model_file = \"Meta-Llama-3.1-8B-Instruct.Q5_0.gguf\"\n",
    "model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply RAG wih chroma and llama 3.1 8B 5bit quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(llm=llm, memory_key=\"chat_history\", output_key='answer', return_messages=True)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "# retriever.search_kwargs = {\"filter\":{\"type\":\"filter\"},\"k\": 3}\n",
    "\n",
    "conversational_retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\n",
      "\n",
      "You know, it all comes down to the polls. We know that President Biden wanted to see some of this data. Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump. There are some bright spots I think on the democratic side for the Democrats and some of the battlegrounds but make no mistake. The numbers that I've seen show that she has a very stiff challenge ahead if she's the nominee in these key battlegrounds. That's exactly right, David, this does not solve all of Democrats, the Democrats problems. In fact, it could make them worse in some states. If you believe the very early battleground state polls, there hasn't been much out there. We have seen a national poll that came out just in the couple of the last couple of days that had a dead tie between Joe Biden. I'm sorry, between Donald Trump and Kamala Harris. And we also had some new polling out today from ABC news and IPSOS that showed favorability ratings for some of the major candidates check that out. Kamala Harris's number a little bit better than Joe Biden, but not as good as Donald Trump. Trump got a significant boost out of his convention and out of the assassination attempt. So these numbers still have to be concerning to Democrats, but they've got a lot of time to try to build up Harris. And it was significant in your interview with Donna right there that they are going to continue to push to have virtual roll call ahead of the national convention. The deadline is actually going to be August 7th according to Donna, who is part of the Democratic National Committee that pushes things up and that tells you some of the urgency now among Democrats, they want to get this squared away to stop any challenges that could happen on the convention floor that remains the plan of the Democratic National Committee at this hour to try to get this all wrapped up in a bowl before they arrive in Chicago. That is going to be a tough task with Senator Manchin and others circling about other opportunities. We\n",
      "\n",
      "Question: tell me about python's ecosystem of third party libraries it uses?\n",
      "Helpful Answer:\u001b[0m\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2000 is out of bounds for axis 0 with size 2000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me about python\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ecosystem of third party libraries it uses?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconversational_retrieval_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:168\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    166\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[0;32m    167\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[1;32m--> 168\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    169\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_inputs\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:605\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    139\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:249\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\llm.py:128\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain\\chains\\llm.py:140\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    138\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    141\u001b[0m         prompts,\n\u001b[0;32m    142\u001b[0m         stop,\n\u001b[0;32m    143\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    148\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\language_models\\llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    702\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\language_models\\llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    869\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    870\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    881\u001b[0m     ]\n\u001b[1;32m--> 882\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    883\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    884\u001b[0m     )\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\language_models\\llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    739\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    741\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\language_models\\llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    719\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 727\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    728\u001b[0m                 prompts,\n\u001b[0;32m    729\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    730\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    731\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    732\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    733\u001b[0m             )\n\u001b[0;32m    734\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    736\u001b[0m         )\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1431\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1430\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1433\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1434\u001b[0m     )\n\u001b[0;32m   1435\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    289\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m     ):\n\u001b[0;32m    294\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    342\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    343\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    344\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    345\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[0;32m    346\u001b[0m     )\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\llama_cpp\\llama.py:1207\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1205\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1206\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1208\u001b[0m     prompt_tokens,\n\u001b[0;32m   1209\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1210\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1211\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1212\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1213\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1214\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1215\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1216\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1217\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1218\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1219\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1220\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1221\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1222\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1223\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1224\u001b[0m ):\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\llama_cpp\\llama.py:799\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 799\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    801\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    802\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    803\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    817\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    818\u001b[0m         )\n",
      "File \u001b[1;32me:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\llama_cpp\\llama.py:650\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    646\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_vocab\n\u001b[0;32m    647\u001b[0m     logits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mctypeslib\u001b[38;5;241m.\u001b[39mas_array(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mget_logits(), shape\u001b[38;5;241m=\u001b[39m(rows \u001b[38;5;241m*\u001b[39m cols,)\n\u001b[0;32m    649\u001b[0m     )\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[::] \u001b[38;5;241m=\u001b[39m logits\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# Update n_tokens\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_tokens\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2000 is out of bounds for axis 0 with size 2000"
     ]
    }
   ],
   "source": [
    "query = \"tell me about python's ecosystem of third party libraries it uses?\"\n",
    "response = conversational_retrieval_chain(\n",
    "    {\"question\": query}\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking chroma db content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking  {'ids': ['1aec14c9-67e3-45fa-a992-ad720c4ae2f8', '40ded77d-8766-4b33-9ad1-39bed0a3dc05', '833dcbe4-c90a-4a23-af46-c451c172a9d0', 'd917d493-fd92-43d1-864e-98a4a1bd00c6', 'f95035ab-603b-4eb4-8cba-50b53028aa5e'], 'embeddings': None, 'metadatas': [{'source': './transcription_results\\\\transcript2.txt'}, {'source': './transcription_results\\\\transcript2.txt'}, {'source': './transcription_results\\\\transcript2.txt'}, {'source': './transcription_results\\\\transcript1.txt'}, {'source': './transcription_results\\\\transcript1.txt'}], 'documents': [\"Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\", \"Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\", \"Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\", \"You know, it all comes down to the polls. We know that President Biden wanted to see some of this data. Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump. There are some bright spots I think on the democratic side for the Democrats and some of the battlegrounds but make no mistake. The numbers that I've seen show that she has a very stiff challenge ahead if she's the nominee in these key battlegrounds. That's exactly right, David, this does not solve all of Democrats, the Democrats problems. In fact, it could make them worse in some states. If you believe the very early battleground state polls, there hasn't been much out there. We have seen a national poll that came out just in the couple of the last couple of days that had a dead tie between Joe Biden. I'm sorry, between Donald Trump and Kamala Harris. And we also had some new polling out today from ABC news and IPSOS that showed favorability ratings for some of the major candidates check that out. Kamala Harris's number a little bit better than Joe Biden, but not as good as Donald Trump. Trump got a significant boost out of his convention and out of the assassination attempt. So these numbers still have to be concerning to Democrats, but they've got a lot of time to try to build up Harris. And it was significant in your interview with Donna right there that they are going to continue to push to have virtual roll call ahead of the national convention. The deadline is actually going to be August 7th according to Donna, who is part of the Democratic National Committee that pushes things up and that tells you some of the urgency now among Democrats, they want to get this squared away to stop any challenges that could happen on the convention floor that remains the plan of the Democratic National Committee at this hour to try to get this all wrapped up in a bowl before they arrive in Chicago. That is going to be a tough task with Senator Manchin and others circling about other opportunities. We\", \"You know, it all comes down to the polls. We know that President Biden wanted to see some of this data. Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump. There are some bright spots I think on the democratic side for the Democrats and some of the battlegrounds but make no mistake. The numbers that I've seen show that she has a very stiff challenge ahead if she's the nominee in these key battlegrounds. That's exactly right, David, this does not solve all of Democrats, the Democrats problems. In fact, it could make them worse in some states. If you believe the very early battleground state polls, there hasn't been much out there. We have seen a national poll that came out just in the couple of the last couple of days that had a dead tie between Joe Biden. I'm sorry, between Donald Trump and Kamala Harris. And we also had some new polling out today from ABC news and IPSOS that showed favorability ratings for some of the major candidates check that out. Kamala Harris's number a little bit better than Joe Biden, but not as good as Donald Trump. Trump got a significant boost out of his convention and out of the assassination attempt. So these numbers still have to be concerning to Democrats, but they've got a lot of time to try to build up Harris. And it was significant in your interview with Donna right there that they are going to continue to push to have virtual roll call ahead of the national convention. The deadline is actually going to be August 7th according to Donna, who is part of the Democratic National Committee that pushes things up and that tells you some of the urgency now among Democrats, they want to get this squared away to stop any challenges that could happen on the convention floor that remains the plan of the Democratic National Committee at this hour to try to get this all wrapped up in a bowl before they arrive in Chicago. That is going to be a tough task with Senator Manchin and others circling about other opportunities. We\"], 'uris': None, 'data': None, 'included': ['metadatas', 'documents']}\n",
      "_________________\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings and vector database\n",
    "embedding_model_name = \"intfloat/e5-large-v2\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "persist_directory = './chroma_db'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=hf_embeddings)\n",
    "\n",
    "# Retrieve all documents\n",
    "all_docs = vectordb.get()\n",
    "\n",
    "print(\"checking \",all_docs)\n",
    "print(\"_________________\")\n",
    "# Print the contents of all documents\n",
    "# for i, doc in enumerate(all_docs):\n",
    "#     print(f\"Document {i+1}:\\n{doc['page_content']}\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs chunks [Document(metadata={'source': './transcription_results\\\\transcript1.txt'}, page_content=\"You know, it all comes down to the polls. We know that President Biden wanted to see some of this data. Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump. There are some bright spots I think on the democratic side for the Democrats and some of the battlegrounds but make no mistake. The numbers that I've seen show that she has a very stiff challenge ahead if she's the nominee in these key battlegrounds. That's exactly right, David, this does not solve all of Democrats, the Democrats problems. In fact, it could make them worse in some states. If you believe the very early battleground state polls, there hasn't been much out there. We have seen a national poll that came out just in the couple of the last couple of days that had a dead tie between Joe Biden. I'm sorry, between Donald Trump and Kamala Harris. And we also had some new polling out today from ABC news and IPSOS that showed favorability ratings for some of the major candidates check that out. Kamala Harris's number a little bit better than Joe Biden, but not as good as Donald Trump. Trump got a significant boost out of his convention and out of the assassination attempt. So these numbers still have to be concerning to Democrats, but they've got a lot of time to try to build up Harris. And it was significant in your interview with Donna right there that they are going to continue to push to have virtual roll call ahead of the national convention. The deadline is actually going to be August 7th according to Donna, who is part of the Democratic National Committee that pushes things up and that tells you some of the urgency now among Democrats, they want to get this squared away to stop any challenges that could happen on the convention floor that remains the plan of the Democratic National Committee at this hour to try to get this all wrapped up in a bowl before they arrive in Chicago. That is going to be a tough task with Senator Manchin and others circling about other opportunities. We\")]\n",
      "all docs  None\n",
      "docs chunks [Document(metadata={'source': './transcription_results\\\\transcript2.txt'}, page_content=\"Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\")]\n",
      "all docs  None\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Directory for Chroma database\n",
    "persist_directory = './chroma_db'\n",
    "\n",
    "# Directory containing transcription results\n",
    "transcription_dir = \"./transcription_results\"\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "# Load and process documents\n",
    "for file_name in os.listdir(transcription_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(transcription_dir, file_name)\n",
    "        loader = TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=2)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        print(\"docs chunks\" , docs)\n",
    "        a = all_docs.extend(docs)\n",
    "        print(\"all docs \", a)\n",
    "# # Initialize Chroma vector database with the documents\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=all_docs,\n",
    "#     embedding=hf_embeddings,\n",
    "#     persist_directory=persist_directory\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan-T5-large model loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When chroma db is already setup/created before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    ")\n",
    "\n",
    "persist_directory = './chroma_db'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RAG with chroma and FlanT5 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a pipeline for the HuggingFace model\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap the pipeline in a LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n",
    "# Initialize memory for conversational context\n",
    "\n",
    "memory = ConversationBufferMemory(llm=llm, memory_key=\"chat_history\", output_key='answer', return_messages=True)\n",
    "\n",
    "# Create ConversationalRetrievalChain\n",
    "conversational_retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Python, a high level interpreted programming language famous for its Zen Light code. It's arguably the most popular language in the world because it's easy to learn yet practical for serious projects. In fact, you're watching this youtube video in a Python web application right now. It was created by Guido Van Rossum and released in 1991 who named it after Monty Python's Flying Circus, which is why you'll sometimes find spam and eggs instead of foo and bar and code samples. It's commonly used to build server site application like web apps with the Django framework and is the language of choice for big data analysis and machine learning. Many students choose Python to start learning to code because of its emphasis on readability. As outlined by the Zen of Python, beautiful is better than ugly while explicit is better than implicit. Python is very simple but avoids the temptation to Sprinkle in magic that causes ambiguity. Its code is often organized into notebooks where individual cells can be executed then documented in the same place. We're currently version three of the language and you can get started by creating a file that ends in dot Py or do IP Y and B to create an interactive notebook, create a variable by setting a name equal to a value it's strongly typed, which means values won't change in unexpected ways but dynamic. So type annotations are not required. The syntax is highly efficient allowing you to declare multiple variables on a single line and define tuples lists and dictionaries with a literal syntax, semicolons are not required. And if you use them and experience Python will say that your code is not Python instead of semicolons. Python uses indentation to terminate or determine the scope of a line of code, define a function with a de keyword, then indent the next line usually by four spaces to define the function body, we might then add a four loop to it and indent that by another four spaces. This eliminates the need for curly braces and semicolons found in many other languages. Python is a multi paradigm language. We can apply functional programming patterns with things like anonymous functions using lambda also uses objects as an abstraction for data allowing you to implement object oriented patterns with things like classes and inheritance. It also has a huge ecosystem of third party libraries such as deep learning frameworks like tensorflow and wrappers for many high performance low level packages like open computer vision which are most often installed with the PIP package manager. This has been the Python programming language in 100 seconds, hit the like button if you want to see more short videos like this. Thanks for watching and I will see you in the next one.\n",
      "\n",
      "You know, it all comes down to the polls. We know that President Biden wanted to see some of this data. Kamala Harris performs about the same, maybe a little better nationally up against Donald Trump. There are some bright spots I think on the democratic side for the Democrats and some of the battlegrounds but make no mistake. The numbers that I've seen show that she has a very stiff challenge ahead if she's the nominee in these key battlegrounds. That's exactly right, David, this does not solve all of Democrats, the Democrats problems. In fact, it could make them worse in some states. If you believe the very early battleground state polls, there hasn't been much out there. We have seen a national poll that came out just in the couple of the last couple of days that had a dead tie between Joe Biden. I'm sorry, between Donald Trump and Kamala Harris. And we also had some new polling out today from ABC news and IPSOS that showed favorability ratings for some of the major candidates check that out. Kamala Harris's number a little bit better than Joe Biden, but not as good as Donald Trump. Trump got a significant boost out of his convention and out of the assassination attempt. So these numbers still have to be concerning to Democrats, but they've got a lot of time to try to build up Harris. And it was significant in your interview with Donna right there that they are going to continue to push to have virtual roll call ahead of the national convention. The deadline is actually going to be August 7th according to Donna, who is part of the Democratic National Committee that pushes things up and that tells you some of the urgency now among Democrats, they want to get this squared away to stop any challenges that could happen on the convention floor that remains the plan of the Democratic National Committee at this hour to try to get this all wrapped up in a bowl before they arrive in Chicago. That is going to be a tough task with Senator Manchin and others circling about other opportunities. We\n",
      "\n",
      "Question: tell me about python's ecosystem of third party libraries it uses?\n",
      "Helpful Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects_working\\Audio_transcript\\newEnvAudio\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: deep learning frameworks like tensorflow and wrappers for many high performance low level\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me about python's ecosystem of third party libraries it uses?\"\n",
    "response = conversational_retrieval_chain(\n",
    "    {\"question\": query}\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
